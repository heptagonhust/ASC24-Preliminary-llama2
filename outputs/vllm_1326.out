2023-12-26 13:49:47,122	INFO usage_lib.py:421 -- Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2023-12-26 13:49:47,122	INFO scripts.py:744 -- Local node IP: 192.168.251.101
2023-12-26 13:49:50,224	SUCC scripts.py:781 -- --------------------
2023-12-26 13:49:50,224	SUCC scripts.py:782 -- Ray runtime started.
2023-12-26 13:49:50,224	SUCC scripts.py:783 -- --------------------
2023-12-26 13:49:50,224	INFO scripts.py:785 -- Next steps
2023-12-26 13:49:50,225	INFO scripts.py:788 -- To add another node to this Ray cluster, run
2023-12-26 13:49:50,225	INFO scripts.py:791 --   ray start --address='192.168.251.101:6379'
2023-12-26 13:49:50,225	INFO scripts.py:800 -- To connect to this Ray cluster:
2023-12-26 13:49:50,227	INFO scripts.py:802 -- import ray
2023-12-26 13:49:50,227	INFO scripts.py:803 -- ray.init(_node_ip_address='192.168.251.101')
2023-12-26 13:49:50,227	INFO scripts.py:834 -- To terminate the Ray runtime, run
2023-12-26 13:49:50,227	INFO scripts.py:835 --   ray stop
2023-12-26 13:49:50,227	INFO scripts.py:838 -- To view the status of the cluster, use
2023-12-26 13:49:50,227	INFO scripts.py:839 --   ray status
2023-12-26 13:49:52,179	INFO scripts.py:926 -- Local node IP: 192.168.251.104
2023-12-26 13:49:52,213	SUCC scripts.py:939 -- --------------------
2023-12-26 13:49:52,213	SUCC scripts.py:940 -- Ray runtime started.
2023-12-26 13:49:52,213	SUCC scripts.py:941 -- --------------------
2023-12-26 13:49:52,213	INFO scripts.py:943 -- To terminate the Ray runtime, run
2023-12-26 13:49:52,213	INFO scripts.py:944 --   ray stop
2023-12-26 13:49:52,222	INFO scripts.py:926 -- Local node IP: 192.168.251.106
2023-12-26 13:49:52,297	SUCC scripts.py:939 -- --------------------
2023-12-26 13:49:52,297	SUCC scripts.py:940 -- Ray runtime started.
2023-12-26 13:49:52,297	SUCC scripts.py:941 -- --------------------
2023-12-26 13:49:52,297	INFO scripts.py:943 -- To terminate the Ray runtime, run
2023-12-26 13:49:52,297	INFO scripts.py:944 --   ray stop
INFO 12-26 13:50:40 llm_engine.py:73] Initializing an LLM engine with config: model='/data/70B-chat-hf', tokenizer='/data/70B-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, quantization=None, enforce_eager=False, seed=0)
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:110852 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,veth
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:110852 [0] NCCL INFO Bootstrap : Using ens25f0:192.168.250.101<0>
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:110852 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:110852 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:110852 [0] NCCL INFO cudaDriverVersion 12020
[36m(RayWorkerVllm pid=110852)[0m NCCL version 2.18.1+cuda12.1
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO NET/IB : Using [0]mlx4_1:1/IB [1]mlx4_0:1/IB ; OOB ens25f0:192.168.250.103<0>
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Using network IB
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Trees [0] 2/-1/-1->5->4 [1] 4/-1/-1->5->1 [2] -1/-1/-1->5->4 [3] 4/-1/-1->5->0
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO P2P Chunksize set to 131072
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 00/0 : 5[b1000] -> 0[4b000] [send] via NET/IB/1
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 01/0 : 5[b1000] -> 0[4b000] [send] via NET/IB/0
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 02/0 : 5[b1000] -> 0[4b000] [send] via NET/IB/1
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 03/0 : 5[b1000] -> 0[4b000] [send] via NET/IB/0
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Connected all rings
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 01/0 : 5[b1000] -> 1[b1000] [send] via NET/IB/0
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 00/0 : 2[4b000] -> 5[b1000] [receive] via NET/IB/1
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 00/0 : 5[b1000] -> 2[4b000] [send] via NET/IB/1
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 01/0 : 1[b1000] -> 5[b1000] [receive] via NET/IB/0
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 03/0 : 0[4b000] -> 5[b1000] [receive] via NET/IB/0
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 00 : 5[b1000] -> 4[4b000] via SHM/direct/direct
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 01 : 5[b1000] -> 4[4b000] via SHM/direct/direct
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 02 : 5[b1000] -> 4[4b000] via SHM/direct/direct
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Channel 03 : 5[b1000] -> 4[4b000] via SHM/direct/direct
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO Connected all trees
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
[36m(RayWorkerVllm pid=10960, ip=192.168.251.106)[0m hepgpu3:10960:11126 [1] NCCL INFO comm 0x9e78db0 rank 5 nranks 6 cudaDev 1 busId b1000 commId 0x731b3e82c528c79b - Init COMPLETE
[36m(RayWorkerVllm pid=10959, ip=192.168.251.106)[0m hepgpu3:10959:11125 [0] NCCL INFO Setting affinity for GPU 0 to 01,00000000,00000001
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111535 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111535 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111535 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111535 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111535 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,veth[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerVllm pid=394132, ip=192.168.251.104)[0m hepgpu2:394132:394132 [1] NCCL INFO Bootstrap : Using ens25f0:192.168.250.102<0>[32m [repeated 4x across cluster][0m
[36m(RayWorkerVllm pid=394132, ip=192.168.251.104)[0m hepgpu2:394132:394132 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory[32m [repeated 4x across cluster][0m
[36m(RayWorkerVllm pid=394132, ip=192.168.251.104)[0m hepgpu2:394132:394132 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation[32m [repeated 4x across cluster][0m
[36m(RayWorkerVllm pid=394132, ip=192.168.251.104)[0m hepgpu2:394132:394132 [1] NCCL INFO cudaDriverVersion 12020[32m [repeated 4x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_0:1/IB ; OOB ens25f0:192.168.250.101<0>[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111603 [0] NCCL INFO Using network IB[32m [repeated 6x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/5/-1->1->-1 [2] 4/-1/-1->1->0 [3] 0/-1/-1->1->3[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO P2P Chunksize set to 131072[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO Channel 01/0 : 1[b1000] -> 5[b1000] [send] via NET/IB/0[32m [repeated 20x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO Connected all rings[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO Channel 03/0 : 3[b1000] -> 1[b1000] [receive] via NET/IB/0[32m [repeated 23x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO Channel 03 : 1[b1000] -> 0[4b000] via SHM/direct/direct[32m [repeated 20x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO Connected all trees[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO comm 0x8b87cb0 rank 1 nranks 6 cudaDev 1 busId b1000 commId 0x731b3e82c528c79b - Init COMPLETE[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111535 [0] NCCL INFO Setting affinity for GPU 0 to 01,00000000,00000001[32m [repeated 2x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111536 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,veth[32m [repeated 2x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:110853 [1] NCCL INFO Bootstrap : Using ens25f0:192.168.250.101<0>
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:110853 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:110853 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:110853 [1] NCCL INFO cudaDriverVersion 12020
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111603 [0] NCCL INFO Channel 00/02 :    0   1
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111603 [0] NCCL INFO Channel 01/02 :    0   1
[36m(RayWorkerVllm pid=110853)[0m NCCL version 2.18.1+cuda12.1
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111610 [0] NCCL INFO Channel 00/02 :    0   1
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111610 [0] NCCL INFO Channel 01/02 :    0   1
[36m(RayWorkerVllm pid=110852)[0m 
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111610 [0] transport.cc:154 NCCL WARN Cuda failure 'invalid argument'
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111610 [0] NCCL INFO init.cc:1032 -> 1
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111610 [0] NCCL INFO init.cc:1309 -> 1
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111610 [0] NCCL INFO group.cc:64 -> 1 [Async thread]
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:110852 [0] NCCL INFO group.cc:422 -> 1
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:110852 [0] NCCL INFO group.cc:106 -> 1
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111613 [0] NCCL INFO misc/socket.cc:46 -> 3
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111613 [0] NCCL INFO misc/socket.cc:57 -> 3
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111613 [0] NCCL INFO misc/socket.cc:769 -> 3
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111613 [0] NCCL INFO proxy.cc:1333 -> 3
[36m(RayWorkerVllm pid=110852)[0m 
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111613 [0] proxy.cc:1484 NCCL WARN [Service thread] Error encountered progressing operation=Connect, res=3, closing connection
[36m(RayWorkerVllm pid=110852)[0m 
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111613 [0] proxy.cc:1518 NCCL WARN [Proxy Service 0] Failed to execute operation Connect from rank 0, retcode 3
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111612 [1] NCCL INFO Using network IB[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111612 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1[32m [repeated 6x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111612 [1] NCCL INFO P2P Chunksize set to 131072[32m [repeated 6x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111618 [1] NCCL INFO Channel 01/1 : 0[b1000] -> 1[b1000] [send] via NET/IB/0/Shared[32m [repeated 10x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111612 [1] NCCL INFO Connected all rings[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=394132, ip=192.168.251.104)[0m hepgpu2:394132:394332 [1] NCCL INFO Channel 01/1 : 0[b1000] -> 1[b1000] [receive] via NET/IB/0/Shared[32m [repeated 12x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111604 [1] NCCL INFO Channel 01 : 1[b1000] -> 0[4b000] via SHM/direct/direct[32m [repeated 4x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111612 [1] NCCL INFO Connected all trees[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111612 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111612 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111612 [1] NCCL INFO comm 0x1eae33b0 rank 0 nranks 2 cudaDev 1 busId b1000 commId 0x19c2ab7e311c7a6e - Init COMPLETE[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=110852)[0m hepgpu1:110852:111610 [0] NCCL INFO Setting affinity for GPU 0 to 01,00000000,00000001[32m [repeated 3x across cluster][0m
[36m(RayWorkerVllm pid=110853)[0m hepgpu1:110853:111612 [1] NCCL INFO Channel 01/02 :    0   1[32m [repeated 2x across cluster][0m
2023-12-26 13:51:15,495	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/raylet --store_socket_name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=192.168.251.106 --maximum_startup_concurrency=128 --static_resource_list=node:192.168.251.106,1.0,accelerator_type:L40,1,CPU,128,GPU,2,memory,789055401984,object_store_memory,200000000000 "--python_worker_command=/data/asc24llama/miniconda3/envs/llama_new/bin/python /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/workers/default_worker.py --node-ip-address=192.168.251.106 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/raylet --redis-address=None --temp-dir=/tmp/ray --metrics-agent-port=59433 --runtime-env-agent-port=61397 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --runtime-env-agent-port=61397 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-26_13-49-47_123626_106091 --temp-dir=/tmp/ray --webui= --cluster-id=5ccd16392616e9cd15febc227fb446f78d70e698c67589bab90d9515 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" --java_worker_command= --cpp_worker_command= --native_library_path=/data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/cpp/lib --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --log_dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --resource_dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/runtime_resources --metrics-agent-port=59433 --metrics_export_port=60640 --runtime_env_agent_port=61397 --object_store_memory=200000000000 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-26_13-49-47_123626_106091 --labels= --cluster-id=5ccd16392616e9cd15febc227fb446f78d70e698c67589bab90d9515 --num_prestart_python_workers=128 "--dashboard_agent_command=/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/dashboard/agent.py --node-ip-address=192.168.251.106 --metrics-export-port=60640 --dashboard-agent-port=59433 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/raylet --temp-dir=/tmp/ray --session-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --log-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2023-12-26_13-49-47_123626_106091 --gcs-address=192.168.251.101:6379 --minimal" "--runtime_env_agent_command=/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/runtime_env/agent/main.py --node-ip-address=192.168.251.106 --runtime-env-agent-port=61397 --gcs-address=192.168.251.101:6379 --runtime-env-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/runtime_resources --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --log-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --temp-dir=/tmp/ray"` (via SIGTERM)
2023-12-26 13:51:15,632	INFO scripts.py:1121 -- 1/1 stopped.2023-12-26 13:51:15,841	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --logs-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2023-12-26 13:51:15,845	VINFO scripts.py:1091 -- Send termination request to `` (via SIGTERM)
2023-12-26 13:51:15,847	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --logs-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2023-12-26 13:51:15,947	INFO scripts.py:1121 -- 1/2 stopped.2023-12-26 13:51:15,947	INFO scripts.py:1121 -- 2/2 stopped.2023-12-26 13:51:16,100	SUCC scripts.py:1166 -- Stopped all 3 Ray processes.
2023-12-26 13:51:15,696	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/raylet --store_socket_name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=192.168.251.104 --maximum_startup_concurrency=128 --static_resource_list=node:192.168.251.104,1.0,accelerator_type:L40,1,CPU,128,GPU,2,memory,589961719808,object_store_memory,200000000000 "--python_worker_command=/data/asc24llama/miniconda3/envs/llama_new/bin/python /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/workers/default_worker.py --node-ip-address=192.168.251.104 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/raylet --redis-address=None --temp-dir=/tmp/ray --metrics-agent-port=57519 --runtime-env-agent-port=38747 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --runtime-env-agent-port=38747 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-26_13-49-47_123626_106091 --temp-dir=/tmp/ray --webui= --cluster-id=5ccd16392616e9cd15febc227fb446f78d70e698c67589bab90d9515 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" --java_worker_command= --cpp_worker_command= --native_library_path=/data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/cpp/lib --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --log_dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --resource_dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/runtime_resources --metrics-agent-port=57519 --metrics_export_port=59291 --runtime_env_agent_port=38747 --object_store_memory=200000000000 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-26_13-49-47_123626_106091 --labels= --cluster-id=5ccd16392616e9cd15febc227fb446f78d70e698c67589bab90d9515 --num_prestart_python_workers=128 "--dashboard_agent_command=/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/dashboard/agent.py --node-ip-address=192.168.251.104 --metrics-export-port=59291 --dashboard-agent-port=57519 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/raylet --temp-dir=/tmp/ray --session-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --log-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2023-12-26_13-49-47_123626_106091 --gcs-address=192.168.251.101:6379 --minimal" "--runtime_env_agent_command=/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/runtime_env/agent/main.py --node-ip-address=192.168.251.104 --runtime-env-agent-port=38747 --gcs-address=192.168.251.101:6379 --runtime-env-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/runtime_resources --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --log-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --temp-dir=/tmp/ray"` (via SIGTERM)
2023-12-26 13:51:15,850	INFO scripts.py:1121 -- 1/1 stopped.2023-12-26 13:51:16,099	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --logs-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2023-12-26 13:51:16,110	VINFO scripts.py:1091 -- Send termination request to `ray::RayWorkerVllm.execute_method "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""` (via SIGTERM)
2023-12-26 13:51:16,115	VINFO scripts.py:1099 -- Attempted to stop `/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --logs-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5`, but process was already dead.
2023-12-26 13:51:16,660	INFO scripts.py:1121 -- 1/2 stopped.2023-12-26 13:51:17,513	INFO scripts.py:1121 -- 2/2 stopped.2023-12-26 13:51:17,666	SUCC scripts.py:1166 -- Stopped all 3 Ray processes.
2023-12-26 13:51:16,216	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/raylet --store_socket_name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=192.168.251.101 --maximum_startup_concurrency=128 --static_resource_list=node:192.168.251.101,1.0,node:__internal_head__,1.0,accelerator_type:L40,1,CPU,128,GPU,2,memory,822245788672,object_store_memory,200000000000 "--python_worker_command=/data/asc24llama/miniconda3/envs/llama_new/bin/python /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/workers/default_worker.py --node-ip-address=192.168.251.101 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/raylet --redis-address=None --temp-dir=/tmp/ray --metrics-agent-port=42047 --runtime-env-agent-port=42493 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --runtime-env-agent-port=42493 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-26_13-49-47_123626_106091 --temp-dir=/tmp/ray --webui= --cluster-id=5ccd16392616e9cd15febc227fb446f78d70e698c67589bab90d9515 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/data/asc24llama/miniconda3/envs/llama_new/bin/python /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py -Dray.address=192.168.251.101:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/plasma_store -Dray.raylet.socket-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/raylet -Dray.redis.password= -Dray.node-ip=192.168.251.101 -Dray.home=/data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/../.. -Dray.logging.dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs -Dray.session-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/cpp/lib --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --log_dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --resource_dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/runtime_resources --metrics-agent-port=42047 --metrics_export_port=57457 --runtime_env_agent_port=42493 --object_store_memory=200000000000 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-26_13-49-47_123626_106091 --labels= --cluster-id=5ccd16392616e9cd15febc227fb446f78d70e698c67589bab90d9515 --head --num_prestart_python_workers=128 "--dashboard_agent_command=/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/dashboard/agent.py --node-ip-address=192.168.251.101 --metrics-export-port=57457 --dashboard-agent-port=42047 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/sockets/raylet --temp-dir=/tmp/ray --session-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --log-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2023-12-26_13-49-47_123626_106091 --gcs-address=192.168.251.101:6379 --minimal" "--runtime_env_agent_command=/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/runtime_env/agent/main.py --node-ip-address=192.168.251.101 --runtime-env-agent-port=42493 --gcs-address=192.168.251.101:6379 --runtime-env-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/runtime_resources --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --log-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --temp-dir=/tmp/ray" --node-name=192.168.251.101` (via SIGTERM)
2023-12-26 13:51:16,841	INFO scripts.py:1121 -- 1/1 stopped.2023-12-26 13:51:17,077	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/autoscaler/_private/monitor.py --logs-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=192.168.251.101:6379 --monitor-ip=192.168.251.101` (via SIGTERM)
2023-12-26 13:51:17,081	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --logs-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2023-12-26 13:51:17,086	VINFO scripts.py:1091 -- Send termination request to `` (via SIGTERM)
2023-12-26 13:51:17,089	VINFO scripts.py:1099 -- Attempted to stop `/data/asc24llama/miniconda3/envs/llama_new/bin/python -u /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --logs-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5`, but process was already dead.
2023-12-26 13:51:17,095	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_new/bin/python /data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/dashboard/dashboard.py --host=127.0.0.1 --port=8265 --port-retries=0 --temp-dir=/tmp/ray --log-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --session-dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=192.168.251.101:6379 --node-ip-address=192.168.251.101 --minimal` (via SIGTERM)
2023-12-26 13:51:17,353	INFO scripts.py:1121 -- 1/4 stopped.2023-12-26 13:51:17,357	INFO scripts.py:1121 -- 2/4 stopped.2023-12-26 13:51:17,357	INFO scripts.py:1121 -- 3/4 stopped.2023-12-26 13:51:17,649	INFO scripts.py:1121 -- 4/4 stopped.2023-12-26 13:51:17,815	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_new/lib/python3.9/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2023-12-26_13-49-47_123626_106091/logs --config_list=eyJvYmplY3Rfc3BpbGxpbmdfY29uZmlnIjogIntcInR5cGVcIjogXCJmaWxlc3lzdGVtXCIsIFwicGFyYW1zXCI6IHtcImRpcmVjdG9yeV9wYXRoXCI6IFwiL3RtcC9yYXkvc2Vzc2lvbl8yMDIzLTEyLTI2XzEzLTQ5LTQ3XzEyMzYyNl8xMDYwOTFcIn19IiwgImlzX2V4dGVybmFsX3N0b3JhZ2VfdHlwZV9mcyI6IHRydWV9 --gcs_server_port=6379 --metrics-agent-port=42047 --node-ip-address=192.168.251.101 --session-name=session_2023-12-26_13-49-47_123626_106091` (via SIGTERM)
2023-12-26 13:51:18,108	INFO scripts.py:1121 -- 1/1 stopped.2023-12-26 13:51:18,109	SUCC scripts.py:1166 -- Stopped all 6 Ray processes.
