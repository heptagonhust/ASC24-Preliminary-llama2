2023-12-23 09:17:22,355	INFO usage_lib.py:421 -- Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2023-12-23 09:17:22,355	INFO scripts.py:744 -- Local node IP: 192.168.251.101
2023-12-23 09:17:24,849	SUCC scripts.py:781 -- --------------------
2023-12-23 09:17:24,849	SUCC scripts.py:782 -- Ray runtime started.
2023-12-23 09:17:24,849	SUCC scripts.py:783 -- --------------------
2023-12-23 09:17:24,849	INFO scripts.py:785 -- Next steps
2023-12-23 09:17:24,849	INFO scripts.py:788 -- To add another node to this Ray cluster, run
2023-12-23 09:17:24,849	INFO scripts.py:791 --   ray start --address='192.168.251.101:6379'
2023-12-23 09:17:24,849	INFO scripts.py:800 -- To connect to this Ray cluster:
2023-12-23 09:17:24,852	INFO scripts.py:802 -- import ray
2023-12-23 09:17:24,852	INFO scripts.py:803 -- ray.init(_node_ip_address='192.168.251.101')
2023-12-23 09:17:24,852	INFO scripts.py:834 -- To terminate the Ray runtime, run
2023-12-23 09:17:24,852	INFO scripts.py:835 --   ray stop
2023-12-23 09:17:24,852	INFO scripts.py:838 -- To view the status of the cluster, use
2023-12-23 09:17:24,852	INFO scripts.py:839 --   ray status
2023-12-23 09:17:27,374	INFO scripts.py:926 -- Local node IP: 192.168.251.104
2023-12-23 09:17:27,426	SUCC scripts.py:939 -- --------------------
2023-12-23 09:17:27,427	SUCC scripts.py:940 -- Ray runtime started.
2023-12-23 09:17:27,427	SUCC scripts.py:941 -- --------------------
2023-12-23 09:17:27,427	INFO scripts.py:943 -- To terminate the Ray runtime, run
2023-12-23 09:17:27,427	INFO scripts.py:944 --   ray stop
2023-12-23 09:17:27,351	INFO scripts.py:926 -- Local node IP: 192.168.251.105
2023-12-23 09:17:27,433	SUCC scripts.py:939 -- --------------------
2023-12-23 09:17:27,433	SUCC scripts.py:940 -- Ray runtime started.
2023-12-23 09:17:27,433	SUCC scripts.py:941 -- --------------------
2023-12-23 09:17:27,434	INFO scripts.py:943 -- To terminate the Ray runtime, run
2023-12-23 09:17:27,434	INFO scripts.py:944 --   ray stop
INFO 12-23 09:18:19 llm_engine.py:73] Initializing an LLM engine with config: model='/data/70B-chat-hf', tokenizer='/data/70B-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=3quantization=None, enforce_eager=False, seed=0)
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:732677 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,veth
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:732677 [0] NCCL INFO Bootstrap : Using ens25f0:192.168.250.101<0>
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:732677 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:732677 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:732677 [0] NCCL INFO cudaDriverVersion 12020
[36m(RayWorkerVllm pid=732677)[0m NCCL version 2.18.1+cuda12.1
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO NET/IB : Using [0]mlx4_1:1/IB [1]mlx4_0:1/IB ; OOB ens25f0:192.168.250.103<0>
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Using network IB
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 2/-1/-1->3->4 [2] -1/-1/-1->3->2 [3] 2/1/-1->3->-1
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO P2P Chunksize set to 131072
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Channel 00/0 : 3[b1000] -> 4[4b000] [send] via NET/IB/1
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Channel 01/0 : 3[b1000] -> 4[4b000] [send] via NET/IB/0
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Channel 02/0 : 3[b1000] -> 4[4b000] [send] via NET/IB/1
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Channel 03/0 : 3[b1000] -> 4[4b000] [send] via NET/IB/0
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Connected all rings
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Channel 03/0 : 1[b1000] -> 3[b1000] [receive] via NET/IB/0
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Channel 03/0 : 3[b1000] -> 1[b1000] [send] via NET/IB/0
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Channel 01/0 : 4[4b000] -> 3[b1000] [receive] via NET/IB/0
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Channel 00 : 3[b1000] -> 2[4b000] via SHM/direct/direct
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Channel 01 : 3[b1000] -> 2[4b000] via SHM/direct/direct
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Channel 02 : 3[b1000] -> 2[4b000] via SHM/direct/direct
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Channel 03 : 3[b1000] -> 2[4b000] via SHM/direct/direct
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO Connected all trees
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152937 [1] NCCL INFO comm 0x9b87eb0 rank 3 nranks 6 cudaDev 1 busId b1000 commId 0xfccea75dadfb60ec - Init COMPLETE
[36m(RayWorkerVllm pid=152768, ip=192.168.251.105)[0m hepgpu3:152768:152936 [0] NCCL INFO Setting affinity for GPU 0 to 02,00000000,00000002
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:733341 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:733341 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:733341 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:733341 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:733341 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,veth[32m [repeated 9x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:281811 [0] NCCL INFO Bootstrap : Using ens25f0:192.168.250.102<0>[32m [repeated 4x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:281811 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory[32m [repeated 4x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:281811 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation[32m [repeated 4x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:281811 [0] NCCL INFO cudaDriverVersion 12020[32m [repeated 4x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_0:1/IB ; OOB ens25f0:192.168.250.101<0>[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:733429 [0] NCCL INFO Using network IB[32m [repeated 6x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/5/-1->1->-1 [2] 4/-1/-1->1->0 [3] 0/-1/-1->1->3[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO P2P Chunksize set to 131072[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO Channel 01/0 : 1[b1000] -> 5[b1000] [send] via NET/IB/0[32m [repeated 21x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO Connected all rings[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO Channel 03/0 : 3[b1000] -> 1[b1000] [receive] via NET/IB/0[32m [repeated 24x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO Channel 03 : 1[b1000] -> 0[4b000] via SHM/direct/direct[32m [repeated 20x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO Connected all trees[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO threadThresholds 8/8/64 | 48/8/64 | 512 | 512[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO comm 0x99588e0 rank 1 nranks 6 cudaDev 1 busId b1000 commId 0xfccea75dadfb60ec - Init COMPLETE[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:733341 [0] NCCL INFO Setting affinity for GPU 0 to 02,00000000,00000002[32m [repeated 2x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:733342 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^lo,docker,veth[32m [repeated 2x across cluster][0m
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:732678 [1] NCCL INFO Bootstrap : Using ens25f0:192.168.250.101<0>
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:732678 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:732678 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
[36m(RayWorkerVllm pid=732678)[0m hepgpu1:732678:732678 [1] NCCL INFO cudaDriverVersion 12020
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:733429 [0] NCCL INFO Channel 00/02 :    0   1
[36m(RayWorkerVllm pid=732677)[0m hepgpu1:732677:733429 [0] NCCL INFO Channel 01/02 :    0   1
[36m(RayWorkerVllm pid=732678)[0m sb
[36m(RayWorkerVllm pid=732678)[0m 0
[36m(RayWorkerVllm pid=732678)[0m 3
[36m(RayWorkerVllm pid=732678)[0m tensor([[[ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          ...,
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479]],
[36m(RayWorkerVllm pid=732678)[0m         [[ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          ...,
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479]],
[36m(RayWorkerVllm pid=732678)[0m         [[ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          ...,
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479]],
[36m(RayWorkerVllm pid=732678)[0m         ...,
[36m(RayWorkerVllm pid=732678)[0m         [[ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          ...,
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479]],
[36m(RayWorkerVllm pid=732678)[0m         [[ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          ...,
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479]],
[36m(RayWorkerVllm pid=732678)[0m         [[ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          ...,
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479],
[36m(RayWorkerVllm pid=732678)[0m          [ 0.9541,  0.0945,  0.1541,  ..., -0.7124, -2.2285, -1.0479]]],
[36m(RayWorkerVllm pid=732678)[0m NCCL version 2.18.1+cuda12.1
INFO 12-23 09:18:53 llm_engine.py:225] # GPU blocks: 20387, # CPU blocks: 4854
[36m(RayWorkerVllm pid=281812, ip=192.168.251.104)[0m INFO 12-23 09:18:58 model_runner.py:405] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
[36m(RayWorkerVllm pid=281812, ip=192.168.251.104)[0m INFO 12-23 09:18:58 model_runner.py:409] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282023 [0] NCCL INFO Using network IB[32m [repeated 13x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282023 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1[32m [repeated 14x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282023 [0] NCCL INFO P2P Chunksize set to 131072[32m [repeated 14x across cluster][0m
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m hepgpu3:152769:152995 [1] NCCL INFO Channel 01/1 : 0[b1000] -> 1[b1000] [send] via NET/IB/0/Shared[32m [repeated 24x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282023 [0] NCCL INFO Connected all rings[32m [repeated 14x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282015 [0] NCCL INFO Channel 01/1 : 0[4b000] -> 1[4b000] [receive] via NET/IB/1/Shared[32m [repeated 24x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282023 [0] NCCL INFO Channel 01 : 0[4b000] -> 1[b1000] via SHM/direct/direct[32m [repeated 12x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282023 [0] NCCL INFO Connected all trees[32m [repeated 14x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282023 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512[32m [repeated 14x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282023 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer[32m [repeated 14x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282023 [0] NCCL INFO comm 0x18b2a7b0 rank 0 nranks 2 cudaDev 0 busId 4b000 commId 0x4fc3569297aa09b5 - Init COMPLETE[32m [repeated 14x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282023 [0] NCCL INFO Setting affinity for GPU 0 to 02,00000000,00000002[32m [repeated 7x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m hepgpu2:281811:282023 [0] NCCL INFO Channel 01/02 :    0   1[32m [repeated 12x across cluster][0m
[36m(RayWorkerVllm pid=152768, ip=192.168.251.105)[0m sb[32m [repeated 3x across cluster][0m
[36m(RayWorkerVllm pid=152768, ip=192.168.251.105)[0m        device='cuda:0', dtype=torch.float16)[32m [repeated 34x across cluster][0m
[36m(RayWorkerVllm pid=152768, ip=192.168.251.105)[0m tensor([[[ 7.1016, -1.3096,  4.0234,  ...,  1.8887, -2.0801, -4.2383],[32m [repeated 3x across cluster][0m
[36m(RayWorkerVllm pid=152768, ip=192.168.251.105)[0m          [ 7.1016, -1.3096,  4.0234,  ...,  1.8887, -2.0801, -4.2383]]],[32m [repeated 90x across cluster][0m
[36m(RayWorkerVllm pid=152768, ip=192.168.251.105)[0m          ...,[32m [repeated 21x across cluster][0m
[36m(RayWorkerVllm pid=152768, ip=192.168.251.105)[0m         [[ 7.1016, -1.3096,  4.0234,  ...,  1.8887, -2.0801, -4.2383],[32m [repeated 15x across cluster][0m
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m NCCL version 2.18.1+cuda12.1[32m [repeated 3x across cluster][0m
[36m(RayWorkerVllm pid=152768, ip=192.168.251.105)[0m INFO 12-23 09:18:58 model_runner.py:405] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=152768, ip=192.168.251.105)[0m INFO 12-23 09:18:58 model_runner.py:409] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.[32m [repeated 5x across cluster][0m
[36m(RayWorkerVllm pid=732677)[0m sb[32m [repeated 8x across cluster][0m
[36m(RayWorkerVllm pid=732677)[0m 2[32m [repeated 44x across cluster][0m
[36m(RayWorkerVllm pid=732677)[0m tensor([[[ 0.5059, -0.2191,  0.5181,  ..., -0.0091, -0.2430, -0.7988]],[32m [repeated 2x across cluster][0m
[36m(RayWorkerVllm pid=152768, ip=192.168.251.105)[0m         [[-0.2600,  0.3228,  0.7412,  ..., -0.4539,  1.0146,  0.1484]]],[32m [repeated 16x across cluster][0m
[36m(RayWorkerVllm pid=732677)[0m         [[ 0.5059, -0.2191,  0.5181,  ..., -0.0091, -0.2430, -0.7988]]],[32m [repeated 10x across cluster][0m
2023-12-23 09:19:02,377	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/raylet --store_socket_name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=192.168.251.104 --maximum_startup_concurrency=128 --static_resource_list=node:192.168.251.104,1.0,accelerator_type:L40,1,CPU,128,GPU,2,memory,591174754304,object_store_memory,200000000000 "--python_worker_command=/data/asc24llama/miniconda3/envs/llama_test/bin/python /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/workers/default_worker.py --node-ip-address=192.168.251.104 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/raylet --redis-address=None --temp-dir=/tmp/ray --metrics-agent-port=61108 --runtime-env-agent-port=43256 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --runtime-env-agent-port=43256 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-23_09-17-22_356188_727902 --temp-dir=/tmp/ray --webui= --cluster-id=71be9480422304de1789ab23f6a8021d06e0e78ebf97654344b7fbc7 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" --java_worker_command= --cpp_worker_command= --native_library_path=/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/cpp/lib --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --log_dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --resource_dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/runtime_resources --metrics-agent-port=61108 --metrics_export_port=61459 --runtime_env_agent_port=43256 --object_store_memory=200000000000 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-23_09-17-22_356188_727902 --labels= --cluster-id=71be9480422304de1789ab23f6a8021d06e0e78ebf97654344b7fbc7 --num_prestart_python_workers=128 "--dashboard_agent_command=/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/dashboard/agent.py --node-ip-address=192.168.251.104 --metrics-export-port=61459 --dashboard-agent-port=61108 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/raylet --temp-dir=/tmp/ray --session-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --log-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2023-12-23_09-17-22_356188_727902 --gcs-address=192.168.251.101:6379 --minimal" "--runtime_env_agent_command=/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/runtime_env/agent/main.py --node-ip-address=192.168.251.104 --runtime-env-agent-port=43256 --gcs-address=192.168.251.101:6379 --runtime-env-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/runtime_resources --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --log-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --temp-dir=/tmp/ray"` (via SIGTERM)
2023-12-23 09:19:02,469	INFO scripts.py:1121 -- 1/1 stopped.2023-12-23 09:19:02,701	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --logs-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2023-12-23 09:19:02,706	VINFO scripts.py:1091 -- Send termination request to `` (via SIGTERM)
2023-12-23 09:19:02,709	VINFO scripts.py:1099 -- Attempted to stop `/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --logs-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5`, but process was already dead.
2023-12-23 09:19:03,249	INFO scripts.py:1121 -- 1/2 stopped.2023-12-23 09:19:03,742	INFO scripts.py:1121 -- 2/2 stopped.2023-12-23 09:19:03,897	SUCC scripts.py:1166 -- Stopped all 3 Ray processes.
2023-12-23 09:19:02,407	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/raylet --store_socket_name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=192.168.251.105 --maximum_startup_concurrency=128 --static_resource_list=node:192.168.251.105,1.0,accelerator_type:L40,1,CPU,128,GPU,2,memory,585127186432,object_store_memory,200000000000 "--python_worker_command=/data/asc24llama/miniconda3/envs/llama_test/bin/python /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/workers/default_worker.py --node-ip-address=192.168.251.105 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/raylet --redis-address=None --temp-dir=/tmp/ray --metrics-agent-port=50185 --runtime-env-agent-port=61547 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --runtime-env-agent-port=61547 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-23_09-17-22_356188_727902 --temp-dir=/tmp/ray --webui= --cluster-id=71be9480422304de1789ab23f6a8021d06e0e78ebf97654344b7fbc7 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" --java_worker_command= --cpp_worker_command= --native_library_path=/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/cpp/lib --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --log_dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --resource_dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/runtime_resources --metrics-agent-port=50185 --metrics_export_port=48343 --runtime_env_agent_port=61547 --object_store_memory=200000000000 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-23_09-17-22_356188_727902 --labels= --cluster-id=71be9480422304de1789ab23f6a8021d06e0e78ebf97654344b7fbc7 --num_prestart_python_workers=128 "--dashboard_agent_command=/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/dashboard/agent.py --node-ip-address=192.168.251.105 --metrics-export-port=48343 --dashboard-agent-port=50185 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/raylet --temp-dir=/tmp/ray --session-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --log-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2023-12-23_09-17-22_356188_727902 --gcs-address=192.168.251.101:6379 --minimal" "--runtime_env_agent_command=/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/runtime_env/agent/main.py --node-ip-address=192.168.251.105 --runtime-env-agent-port=61547 --gcs-address=192.168.251.101:6379 --runtime-env-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/runtime_resources --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --log-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --temp-dir=/tmp/ray"` (via SIGTERM)
2023-12-23 09:19:02,590	INFO scripts.py:1121 -- 1/1 stopped.2023-12-23 09:19:02,807	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --logs-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2023-12-23 09:19:02,812	VINFO scripts.py:1091 -- Send termination request to `` (via SIGTERM)
2023-12-23 09:19:02,815	VINFO scripts.py:1099 -- Attempted to stop `/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --logs-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5`, but process was already dead.
2023-12-23 09:19:02,822	INFO scripts.py:1121 -- 1/2 stopped.2023-12-23 09:19:03,767	INFO scripts.py:1121 -- 2/2 stopped.2023-12-23 09:19:03,916	SUCC scripts.py:1166 -- Stopped all 3 Ray processes.
2023-12-23 09:19:03,013	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/raylet --store_socket_name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/plasma_store --object_manager_port=0 --min_worker_port=10002 --max_worker_port=19999 --node_manager_port=0 --node_ip_address=192.168.251.101 --maximum_startup_concurrency=128 --static_resource_list=node:192.168.251.101,1.0,node:__internal_head__,1.0,accelerator_type:L40,1,CPU,128,GPU,2,memory,467345759232,object_store_memory,200000000000 "--python_worker_command=/data/asc24llama/miniconda3/envs/llama_test/bin/python /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/workers/default_worker.py --node-ip-address=192.168.251.101 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/raylet --redis-address=None --temp-dir=/tmp/ray --metrics-agent-port=52088 --runtime-env-agent-port=54202 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --runtime-env-agent-port=54202 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-23_09-17-22_356188_727902 --temp-dir=/tmp/ray --webui= --cluster-id=71be9480422304de1789ab23f6a8021d06e0e78ebf97654344b7fbc7 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER" "--java_worker_command=/data/asc24llama/miniconda3/envs/llama_test/bin/python /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/workers/setup_worker.py -Dray.address=192.168.251.101:6379 -Dray.raylet.node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER -Dray.object-store.socket-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/plasma_store -Dray.raylet.socket-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/raylet -Dray.redis.password= -Dray.node-ip=192.168.251.101 -Dray.home=/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/../.. -Dray.logging.dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs -Dray.session-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 RAY_WORKER_DYNAMIC_OPTION_PLACEHOLDER io.ray.runtime.runner.worker.DefaultWorker" --cpp_worker_command= --native_library_path=/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/cpp/lib --temp_dir=/tmp/ray --session_dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --log_dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --resource_dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/runtime_resources --metrics-agent-port=52088 --metrics_export_port=56721 --runtime_env_agent_port=54202 --object_store_memory=200000000000 --plasma_directory=/dev/shm --ray-debugger-external=0 --gcs-address=192.168.251.101:6379 --session-name=session_2023-12-23_09-17-22_356188_727902 --labels= --cluster-id=71be9480422304de1789ab23f6a8021d06e0e78ebf97654344b7fbc7 --head --num_prestart_python_workers=128 "--dashboard_agent_command=/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/dashboard/agent.py --node-ip-address=192.168.251.101 --metrics-export-port=56721 --dashboard-agent-port=52088 --listen-port=52365 --node-manager-port=RAY_NODE_MANAGER_PORT_PLACEHOLDER --object-store-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/plasma_store --raylet-name=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/sockets/raylet --temp-dir=/tmp/ray --session-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --log-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --session-name=session_2023-12-23_09-17-22_356188_727902 --gcs-address=192.168.251.101:6379 --minimal" "--runtime_env_agent_command=/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/runtime_env/agent/main.py --node-ip-address=192.168.251.101 --runtime-env-agent-port=54202 --gcs-address=192.168.251.101:6379 --runtime-env-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/runtime_resources --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --log-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --temp-dir=/tmp/ray" --node-name=192.168.251.101` (via SIGTERM)
2023-12-23 09:19:03,633	INFO scripts.py:1121 -- 1/1 stopped.2023-12-23 09:19:03,903	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/autoscaler/_private/monitor.py --logs-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=192.168.251.101:6379 --monitor-ip=192.168.251.101` (via SIGTERM)
2023-12-23 09:19:03,904	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --logs-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2023-12-23 09:19:03,910	VINFO scripts.py:1091 -- Send termination request to `` (via SIGTERM)
2023-12-23 09:19:03,910	VINFO scripts.py:1091 -- Send termination request to `` (via SIGTERM)
2023-12-23 09:19:03,914	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_test/bin/python -u /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/log_monitor.py --session-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --logs-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --gcs-address=192.168.251.101:6379 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5` (via SIGTERM)
2023-12-23 09:19:03,920	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_test/bin/python /data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/dashboard/dashboard.py --host=127.0.0.1 --port=8265 --port-retries=0 --temp-dir=/tmp/ray --log-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --session-dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902 --logging-rotate-bytes=536870912 --logging-rotate-backup-count=5 --gcs-address=192.168.251.101:6379 --node-ip-address=192.168.251.101 --minimal` (via SIGTERM)
2023-12-23 09:19:04,137	INFO scripts.py:1121 -- 1/5 stopped.2023-12-23 09:19:04,349	INFO scripts.py:1121 -- 2/5 stopped.2023-12-23 09:19:05,307	INFO scripts.py:1121 -- 3/5 stopped.2023-12-23 09:19:05,479	INFO scripts.py:1121 -- 4/5 stopped.2023-12-23 09:19:05,479	INFO scripts.py:1121 -- 5/5 stopped.2023-12-23 09:19:05,643	VINFO scripts.py:1091 -- Send termination request to `/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/session_2023-12-23_09-17-22_356188_727902/logs --config_list=eyJvYmplY3Rfc3BpbGxpbmdfY29uZmlnIjogIntcInR5cGVcIjogXCJmaWxlc3lzdGVtXCIsIFwicGFyYW1zXCI6IHtcImRpcmVjdG9yeV9wYXRoXCI6IFwiL3RtcC9yYXkvc2Vzc2lvbl8yMDIzLTEyLTIzXzA5LTE3LTIyXzM1NjE4OF83Mjc5MDJcIn19IiwgImlzX2V4dGVybmFsX3N0b3JhZ2VfdHlwZV9mcyI6IHRydWV9 --gcs_server_port=6379 --metrics-agent-port=52088 --node-ip-address=192.168.251.101 --session-name=session_2023-12-23_09-17-22_356188_727902` (via SIGTERM)
2023-12-23 09:19:05,876	INFO scripts.py:1121 -- 1/1 stopped.2023-12-23 09:19:05,876	SUCC scripts.py:1166 -- Stopped all 7 Ray processes.
