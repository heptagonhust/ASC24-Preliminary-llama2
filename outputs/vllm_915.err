[2023-12-23 09:17:27,426 I 281655 281655] global_state_accessor.cc:374: This node has an IP address of 192.168.251.104, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.
[2023-12-23 09:17:27,433 I 152612 152612] global_state_accessor.cc:374: This node has an IP address of 192.168.251.105, but we cannot find a local Raylet with the same address. This can happen when you connect to the Ray cluster with a different IP address or when connecting to a container.
2023-12-23 09:17:32,885	INFO worker.py:1489 -- Connecting to existing Ray cluster at address: 192.168.251.101:6379...
2023-12-23 09:17:32,909	INFO worker.py:1673 -- Connected to Ray cluster.
[36m(RayWorkerVllm pid=152769, ip=192.168.251.105)[0m [W ProcessGroupNCCL.cpp:1856] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())
[36m(RayWorkerVllm pid=732678)[0m [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[36m(RayWorkerVllm pid=152768, ip=192.168.251.105)[0m [W ProcessGroupNCCL.cpp:1856] Warning: 0NCCL_AVOID_RECORD_STREAMS=1 has no effect for point-to-point collectives. (function operator())[32m [repeated 7x across cluster][0m
Traceback (most recent call last):
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm-llama-test.py", line 4, in <module>
    llm = LLM(model = "/data/70B-chat-hf", tensor_parallel_size=2, pipeline_parallel_size=3)
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/entrypoints/llm.py", line 105, in __init__
    self.llm_engine = LLMEngine.from_engine_args(engine_args)
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/engine/llm_engine.py", line 266, in from_engine_args
    engine = cls(*engine_configs,
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/engine/llm_engine.py", line 115, in __init__
    self._init_cache()
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/engine/llm_engine.py", line 248, in _init_cache
    self._run_workers("warm_up_model")
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/engine/llm_engine.py", line 773, in _run_workers
    self._run_workers_in_batch(workers, method, *args, **kwargs))
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/engine/llm_engine.py", line 750, in _run_workers_in_batch
    all_outputs = ray.get(all_outputs)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/auto_init_hook.py", line 24, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/_private/worker.py", line 2563, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(RuntimeError): [36mray::RayWorkerVllm.execute_method()[39m (pid=152769, ip=192.168.251.105, actor_id=9fb92d945169b9db6e20543d01000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f9765ef8340>)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/model_executor/models/llama.py", line 329, in forward
    send_to_next_pp_rank(hidden_states)
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/model_executor/parallel_utils/pipeline_parallel/communication.py", line 18, in send_to_next_pp_rank
    print(tensor)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/torch/_tensor.py", line 431, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/torch/_tensor_str.py", line 664, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/torch/_tensor_str.py", line 595, in _str_intern
    tensor_str = _tensor_str(self, indent)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/torch/_tensor_str.py", line 347, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/torch/_tensor_str.py", line 137, in __init__
    nonzero_finite_vals = torch.masked_select(
RuntimeError: CUDA error: operation not permitted when stream is capturing
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

[36mray::RayWorkerVllm.execute_method()[39m (pid=152769, ip=192.168.251.105, actor_id=9fb92d945169b9db6e20543d01000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f9765ef8340>)
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/engine/ray_utils.py", line 31, in execute_method
    return executor(*args, **kwargs)
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/worker/worker.py", line 124, in warm_up_model
    self.model_runner.capture_model(self.gpu_cache)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/worker/model_runner.py", line 438, in capture_model
    graph_runner.capture(
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/worker/model_runner.py", line 485, in capture
    hidden_states = self.model(
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/torch/cuda/graphs.py", line 197, in __exit__
    self.cuda_graph.capture_end()
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/torch/cuda/graphs.py", line 88, in capture_end
    super().capture_end()
RuntimeError: CUDA error: operation failed due to a previous error during capture
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
[36m(RayWorkerVllm pid=281811, ip=192.168.251.104)[0m [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())[32m [repeated 2x across cluster][0m
srun: error: hepgpu1: task 0: Exited with exit code 1
