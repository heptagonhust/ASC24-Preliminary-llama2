Traceback (most recent call last):
  File "/data/asc24llama/miniconda3/envs/llama_test/bin/ray", line 8, in <module>
    sys.exit(main())
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/scripts/scripts.py", line 2498, in main
    return cli()
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/autoscaler/_private/cli_logger.py", line 856, in wrapper
    return f(*args, **kwargs)
  File "/data/asc24llama/miniconda3/envs/llama_test/lib/python3.9/site-packages/ray/scripts/scripts.py", line 764, in start
    raise ConnectionError(
ConnectionError: Ray is trying to start at 192.168.251.101:6379, but is already running at 192.168.251.101:6379. Please specify a different port using the `--port` flag of `ray start` command.
srun: error: hepgpu1: task 0: Exited with exit code 1
Traceback (most recent call last):
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm-llama-test.py", line 1, in <module>
    from vllm import LLM, SamplingParams
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/__init__.py", line 3, in <module>
    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/engine/arg_utils.py", line 6, in <module>
    from vllm.config import (CacheConfig, ModelConfig, ParallelConfig,
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/config.py", line 9, in <module>
    from vllm.utils import get_cpu_memory, is_hip
  File "/data/asc24llama/zbtrs/ASC24-llama2/vllm/utils.py", line 9, in <module>
    from vllm._C import cuda_utils
ModuleNotFoundError: No module named 'vllm._C'
srun: error: hepgpu1: task 0: Exited with exit code 1
